# -*- coding: utf-8 -*-
"""gans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jOfY4AjPFibXZ4gpvqKUP7uzB_fdmg8R

#Import Libraries
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transform
import torchvision.datasets as datasets
import torchvision.utils as utils
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

"""#dataset preparation"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

batch_size = 128 # mini batch size
image_size = 28*28 # image size

transform = transform.Compose([
    transform.ToTensor(), #Â convert images to tensor
    transform.Normalize((0.5,), (0.5,)) # normalization -> compress between -1 and 1
])

# Loading the MNIST dataset
dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)

# load the dataset in batches
dataLoader = DataLoader(dataset, batch_size = batch_size, shuffle = True)

"""#Discriminator"""

class Discriminator(nn.Module): # will try to understand whether the images produced by the generator are real or fake

    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(image_size, 1024), # input: image size, 1024: number of neurons, i.e. output of this layer
            nn.LeakyReLU(0.2), # activation function and slope of 0.2
            nn.Linear(1024, 512), # From 1024 to 512 nodes
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256), # From 512 to 256 nodes
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1), # 256 to 1 node, output layer
            nn.Sigmoid() # sets the output between 0 and 1
            )

    def forward(self, img):
        return self.model(img.view(-1, image_size)) # flatten the image and give it to the model

"""#create generator"""

class Generator(nn.Module):

    def __init__(self, z_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(z_dim, 256), # fully connected layer from input to 256 nodes
            nn.ReLU(),
            nn.Linear(256, 512), # From 256 to 512 nodes
            nn.ReLU(),
            nn.Linear(512, 1024), # From 512 to 1024 nodes
            nn.ReLU(),
            nn.Linear(1024, image_size), # From 1024 to 784 nodes
            nn.Tanh() # output activation function
            )


    def forward(self, x):
        return self.model(x).view(-1, 1, 28, 28) # Converts the output to a 28*28 image

"""#gan training"""

# hyperparameters
lr_g = 0.0002 # genarator learning rate
lr_d = 0.0001 # discriminator learning rate
z_dim = 100 # random noise vector size
epochs = 50 # number of training cycles

# Model initialization: define generator and discriminator
generator = Generator(z_dim).to(device)
discriminator = Discriminator().to(device)

# definition of loss function and optimization algorithms
criterion = nn.BCELoss() # Binary Cross Entropy Loss
g_optimizer = optim.Adam(generator.parameters(), lr = lr_g, betas = (0.5, 0.999)) # generator
d_optimizer = optim.Adam(discriminator.parameters(), lr = lr_d, betas = (0.5, 0.999)) # discriminator


# starting the training cycle
for epoch in range(epochs):
    for i, (real_imgs, _) in enumerate(dataLoader): # uploading images
      real_imgs = real_imgs.to(device)
      batch_size = real_imgs.size(0) # get current batch size
      real_labels = torch.ones(batch_size, 1).to(device) # tag real images 1
      fake_labels = torch.zeros(batch_size, 1).to(device) # Tag fake images as 0

      # Discriminator training
      z = torch.randn(batch_size, z_dim).to(device) # generate random noise vector
      fake_imgs = generator(z) # creation of fake images
      real_loss = criterion(discriminator(real_imgs), real_labels) # loss of real images
      fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels) # loss of fake images
      d_loss = real_loss + fake_loss # total loss

      d_optimizer.zero_grad() # zeroing out gradients
      d_loss.backward()
      d_optimizer.step() # update parameters

      # generator training
      g_loss = criterion(discriminator(fake_imgs), real_labels) # loss of generator
      g_optimizer.zero_grad() # zeroing out gradients
      g_loss.backward()
      g_optimizer.step() # update parameters

    print(f"Epoch {epoch+1}/{epochs}, d_loss: {d_loss.item():.3f}, g_loss: {g_loss.item():.3f}")

"""#model testing and performance evaluation"""

# image generation with random noise
with torch.no_grad():
  z = torch.randn(16, z_dim).to(device) # Generate 16 random noises
  sample_imgs = generator(z).cpu() # Create a fake image with a generator
  grid = np.transpose(utils.make_grid(sample_imgs, nrow = 4, normalize = True), (1, 2, 0)) # arrange images in grid view
  plt.imshow(grid)
  plt.show()